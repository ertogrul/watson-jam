{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hosted with ‚ù§ on Watson Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install gym[atari]\n",
    "# !pip install tensorflow\n",
    "# !conda install -c anaconda pyopengl -y\n",
    "# !pip install PyOpenGL\n",
    "# !conda install -c menpo ffmpeg --yes\n",
    "!pip install pyglet==1.5.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from matplotlib import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "EPISODES   = 1500\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        if load_model:\n",
    "            self.state_size = state_size #Get size of the state\n",
    "            self.action_size = action_size #Get size of the action\n",
    "\n",
    "            #Hyperparameters\n",
    "            self.discount_factor = 0.99 #Disocunt Factor\n",
    "            self.learning_rate = 0.000001 #Learning Rate\n",
    "\n",
    "            #Hyperparameters to adjust the Exploitation-Explore tradeoff\n",
    "            self.epsilon = 0.2 #Setting the epislon (0= Explore, 1= Exploit)\n",
    "            self.epsilon_decay = 0.999999 #Adjusting how our epsilon will decay\n",
    "            self.epsilon_min = 0.2 #Min Epsilon\n",
    "\n",
    "            self.batch_size = 64 #Batch Size for training the neural network\n",
    "            self.train_start = 1000 #If Agent's memory is less, no training is done\n",
    "\n",
    "        else:\n",
    "            self.state_size = state_size #Get size of the state\n",
    "            self.action_size = action_size #Get size of the action\n",
    "\n",
    "            #Hyperparameters\n",
    "            self.discount_factor = 0.99 #Disocunt Factor\n",
    "            self.learning_rate = 0.001 #Learning Rate\n",
    "\n",
    "            #Hyperparameters to adjust the Exploitation-Explore tradeoff\n",
    "            self.epsilon = 1.0 #Setting the epislon (0= Explore, 1= Exploit)\n",
    "            self.epsilon_decay = 0.999 #Adjusting how our epsilon will decay\n",
    "            self.epsilon_min = 0.1 #Min Epsilon\n",
    "\n",
    "            self.batch_size = 64 #Batch Size for training the neural network\n",
    "            self.train_start = 1000 #If Agent's memory is less, no training is done\n",
    "        \n",
    "        # create main replay memory for the agent using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        #Loading weights if load_model=True\n",
    "        if load_model:\n",
    "            self.model.load_weights(\"./pacman.h5\")\n",
    "            \n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))#State is input\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))#Q_Value of each action is Output\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    # save sample <state,action,reward,nest_state> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env.reset()\n",
    "    \n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = Agent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done:\n",
    "            dead = False\n",
    "            while not dead:\n",
    "                env.render()\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # When Pacman dies gives penalty of -100\n",
    "                reward = reward if not dead else -100\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig( \"pacmanTest.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                         len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "        if (e % 50 == 0) & (load_model==False):\n",
    "            agent.model.save_weights(\"pacman.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
